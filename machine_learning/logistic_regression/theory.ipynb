{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression estimates the probability of an event occurring, based on a given data set of independent variables.\n",
    "\n",
    "Since the outcome is a probability, the dependent variable is bounded between 0 and 1. This type of statistical model (also known as logit model) is often used for classification and predictive analytics.\n",
    "\n",
    "It is,\n",
    "* probabilistic classifier\n",
    "* It is a **discriminative model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression has two phases:\n",
    "\n",
    "**training:** We train the system (specifically the weights w and b) using stochastic\n",
    "gradient descent and the cross-entropy loss.\n",
    "\n",
    "**test:** Given a test example x we compute p(y|x) and return the higher probability\n",
    "label y= 1 or y= 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Logistic Function or Sigmoid Function\n",
    "\n",
    "This function maps any real-valued number to the range [0, 1], which we interpret as a probability.\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we use sigmoid function\n",
    "\n",
    "To make a decision on a test instance, the classifier first multiplies each xi by its weight wi, sums up the weighted features,\n",
    "and adds the bias term b. \n",
    "\n",
    "The resulting single number z expresses the weighted sum of the evidence for the class.\n",
    "\n",
    "$$z = w^T x + b$$\n",
    "\n",
    "But nothing forces z to be a legal probability, that is, to lie between 0 and 1. \n",
    "\n",
    "In fact, since weights are real-valued, the output might even be negative; z ranges from −∞ to ∞.\n",
    "\n",
    "To create a probability, we’ll pass z through the **sigmoid function, σ (z)**.\n",
    "\n",
    "The sigmoid has a number of advantages; \n",
    "- it takes a real-valued number and maps it into the range (0, 1), which is just what we want for a probability.\n",
    "-  Because it is nearly linear around 0 but flattens toward the ends, it tends to squash outlier values toward 0 or 1. \n",
    "- And it’s differentiable, which will be handy for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is logit?\n",
    "\n",
    "The input to the sigmoid function, the score $z = w·x + b$ is often called the **logit**. \n",
    "\n",
    "This is because the logit function is the inverse of the sigmoid. \n",
    "\n",
    "The logit function is the log of the odds ratio $\\frac{p}{(1-p)}$\n",
    "\n",
    "$$ logit(p) = \\sigma^{-1} (p) = ln(\\frac{p}{1-p}) $$\n",
    "\n",
    "Using the term **logit** for z is a way of reminding us that by using the sigmoid to turn z (which ranges from−∞ to ∞) into a probability, we are implicitly interpreting z as not just any real-valued number, but as specifically a log odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "In logistic regression, we model the probability of an instance belonging to the positive class as:\n",
    "\n",
    "$$P(y=1|X) = \\sigma(w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$\n",
    "\n",
    "vector form,\n",
    "\n",
    "Now the liear function (w^T x + b) can be written as,\n",
    "\n",
    "$$\\theta^T x' $$\n",
    "\n",
    "- **where x' is individual training instance**\n",
    "\n",
    "- we append the bias term b to the end of the weight vector (w) \n",
    "$$\\theta = [w,b]$$\n",
    "- append 1 to the end of each input vector(x) \n",
    "$$ x' = [x, 1] $$\n",
    "\n",
    "now the probability function becomes,\n",
    "\n",
    "$$P(y=1|x') = \\sigma(\\theta^T x')$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating probability of all training instances at once\n",
    "\n",
    "We can calculate probability vector containing probability of all training instances as,\n",
    "\n",
    "$$X = [[x'], ...]$$\n",
    "\n",
    "where, X = matrix containing all training instances.\n",
    "\n",
    "Probability vector containing probability of all training instances is,\n",
    "\n",
    "$$\\hat{y} = \\sigma(X^T \\theta)$$\n",
    "\n",
    "- x' is column vector\n",
    "- $\\theta$ is also column vector\n",
    "- By performing $X^T$ we converted the column vectors to row vector so that we can perform matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Prediction\n",
    "\n",
    "To make the prediction, we first calculate the linear predictor or logit\n",
    "$$ w^T x + b $$\n",
    "\n",
    "Then we pass this into sigmoid function to get the probability,\n",
    "$$ probability P(y=1|x)= \\sigma(w^T x + b) $$\n",
    "\n",
    "We can set a decision boundary and based on the probability we can make predictions,\n",
    "$$\n",
    "decision(x) = \\hat{y} = \n",
    "  \\begin{cases}\n",
    "    0 & \\text{if } P(y=1|x) < 0.5 \\\\\n",
    "    1 & \\text{if } P(y=1|x) \\ge 0.5 \n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "This is equivalent to,\n",
    "$$\n",
    "\\hat{y} = \n",
    "  \\begin{cases}\n",
    "    0 & \\text{if } (w^T x + b) < 0 \\\\\n",
    "    1 & \\text{if } (w^T x + b) \\ge 0 \n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "## Loss Function (cross-entropy loss)\n",
    "We use the **negative log-likelihood** as loss function. This is modified maximum likelyhood method.\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[y^{(i)}log(\\hat{p}^{(i)}) + (1 - y^{(i)})log(1 - \\hat{p}^{(i)})]$$\n",
    "\n",
    "- m is number of training instances,\n",
    "- $y^i$ is the target value for the current training instance,\n",
    "- $p^i$ is prediction value $\\sigma (\\theta^T x)$ for the current training instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Derivative of loss function:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_j} {J(\\theta)} =  \\frac{1}{m} \\sum\\limits_{i = 1}^{m} (\\sigma(\\theta^Tx^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "Now we can apply various gradient descent methods for finding the optimal weights that minimizes the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
