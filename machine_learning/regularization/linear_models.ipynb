{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25583046-09cc-43a2-9e15-a373501cc57c",
   "metadata": {},
   "source": [
    "# Regularized linear models\n",
    "\n",
    "A good way to reduce overfitting is to regularize the model (i.e., to constrain it).\n",
    "\n",
    "The fewer degrees of freedom it has, the harder it will be for it to overfit the data.\n",
    "\n",
    "For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees.\n",
    "\n",
    "For a linear model, regularization is typically achieved by constraining the weights of the model.\n",
    "\n",
    "**Ridge Regression, Lasso Regression, and Elastic Net** implements three different ways to constrain the weights.\n",
    "\n",
    "**It is important to scale the data (e.g., using a StandardScaler) before performing Ridge Regression, as it is sensitive to the scale of the input features. This is true of most regularized models.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89458923-ee76-4e4e-b839-0e03ad4e434b",
   "metadata": {},
   "source": [
    "# Ridge Regression (Tikhonov regularization)\n",
    "\n",
    "- It is a regularized version of Linear Regression,\n",
    "- a regularization term equal to  $\\alpha\\sum_{i=1}^{n}\\theta_i^2$  is added to the cost function.\n",
    "- This forces the learning algorithm to not only fit the data but also keep the model\n",
    "weights as small as possible.\n",
    "\n",
    "**Note:** the regularization term should only be added to the cost function during training. Once the model is trained, we want to evaluate the model’s performance using the unregularized performance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260eb46-fc51-4686-a922-0c821a0ab7bb",
   "metadata": {},
   "source": [
    "**The hyperparameter α controls how much you want to regularize the model.**\n",
    "\n",
    "If α = 0 then Ridge Regression is just Linear Regression. \n",
    "\n",
    "If α is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean.\n",
    "\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2} \\sum_{i=1}^{n}\\theta_i^2$$\n",
    "\n",
    "**bias term θ0 is not regularized (the sum starts at i = 1, not 0)**\n",
    "\n",
    "If we define w as the vector of feature weights (θ1 to θn), then the regularization term is simply equal to $\\frac{1}{2}(|| w ||_2)^2$, where $|| w ||_2$ represents the l2 norm of the weight vector.\n",
    "\n",
    "**For Gradient Descent, just add αw to the MSE gradient vector.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838daee6-61dd-42e9-8926-d6aaa2cd1d03",
   "metadata": {},
   "source": [
    "## Scikit learn implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b5d7c-dd02-4029-8623-3ad9608cfc0d",
   "metadata": {},
   "source": [
    "**Using closed-form solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0b7fc-e911-4bd9-84f2-a6edd33a2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X,y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a7911-63f0-4a08-a1f8-b2c2ec1ed462",
   "metadata": {},
   "source": [
    "**Using Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fce923-3ae0-4523-b73a-24ebf628e585",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(X, y.ravel()) # The .ravel() function is used to transform a multi-dimensional array into a one-dimensional array.\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6009e297-0e88-4494-978c-23da790a4d4b",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n",
    "Least Absolute Shrinkage and Selection Operator\n",
    "\n",
    "- Similar to to Ridge regression.\n",
    "- It adds the l1 norm of the weight vector as a regularization term to the cost function.\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^{n}|\\theta_i|$$\n",
    "\n",
    "An important characteristic of Lasso Regression is that **it tends to completely eliminate the weights of the least important features** (i.e., set them to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24627e0-7917-44f8-98a7-ddec318eb4d1",
   "metadata": {},
   "source": [
    "**Using Scikit learn Lasso class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b92a5-ddd8-4f6f-bdef-95c70794a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso >>> lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200fd51-943c-42d9-accf-9007960c4d15",
   "metadata": {},
   "source": [
    "**Using SGDClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a447c-dbf5-41b0-8a6a-9d2523a89ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l1\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5700ff-803e-4372-b1bb-b06ed7e7b3f4",
   "metadata": {},
   "source": [
    "# Elastic net\n",
    "\n",
    "It is a middle ground between Ridge Regression and Lasso Regression. \n",
    "\n",
    "The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, we can control the mix ratio r. \n",
    "\n",
    "- When r = 0, Elastic Net is equivalent to Ridge Regression, \n",
    "- When r = 1, it is equivalent to Lasso Regression\n",
    "\n",
    "**Elastic Net cost function**\n",
    "\n",
    "$$J(\\theta) = MSE(\\theta) + r\\alpha \\sum_{i=1}^{n}|\\theta_i| + \\frac{1-r}{2}\\alpha \\sum_{i=1}^{n}\\theta_i^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c04423-4b46-4a56-ae32-5eb52a8e4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5) # l1_ratio corresponds to the mix ratio r\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d0ab3-3c0e-42ca-be53-b85f6ba69bd7",
   "metadata": {},
   "source": [
    "# Conclution\n",
    "\n",
    "It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. \n",
    "\n",
    "Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero as we have discussed. \n",
    "\n",
    "In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
